# -WebNLG-with-GPT2
Colab Link : https://drive.google.com/drive/folders/1OmwLX9IwCAf5d4i7YLnIyjvfqBoMoRie?usp=sharing
RDF-to-Text: Fine-tuning GPT2 with WebNLG Corpus
This project contains 3 Google Colab Notebooks:
 pre-process WebNLG Dataset - Part 1
 fine-tune GPT2 language model with WebNLG Dataset. - Part 2
 generate text with the trained model - Part 3
 evaluate generated text - Part 4
Run them consecutively. 
 
Reference papers:
The WebNLG data (Gardent el al., 2017) was created to promote the development (i) of RDF verbalisers and (ii) of microplanners able to handle a wide range of linguistic constructions.
Gardent, C., Shimorina, A., Narayan, S., & Perez-Beltrachini, L. (2017, September). The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation (pp. 124-133).
GPT2 Language Model : Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.
 
Reference code:
 
https://github.com/MathewAlexander/T5_nlg
https://towardsdatascience.com/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e
https://github.com/cltl/ma-communicative-robots/blob/2021/projects/transformers/Caya's%20project/evaluation_metrics.ipynb
